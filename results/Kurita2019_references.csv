Title,Section,Influential,Year,Authors,Abstract,URL,DOI,Arxiv
Measuring individual differences in implicit cognition: the implicit association test.,methodology,False,1998,A. Greenwald;D. McGhee;J. L. Schwartz,"An implicit association test (IAT) measures differential association of 2 target concepts with an attribute. The 2 concepts appear in a 2-choice task (2-choice task (e.g., flower vs. insect names), and the attribute in a 2nd task (e.g., pleasant vs. unpleasant words for an evaluation attribute). When instructions oblige highly associated categories (e.g., flower + pleasant) to share a response key, performance is faster than when less associated categories (e.g., insect & pleasant) share a key. This performance difference implicitly measures differential association of the 2 concepts with the attribute. In 3 experiments, the IAT was sensitive to (a) near-universal evaluative differences (e.g., flower vs. insect), (b) expected individual differences in evaluative associations (Japanese + pleasant vs. Korean + pleasant for Japanese vs. Korean subjects), and (c) consciously disavowed evaluative differences (Black + pleasant vs. White + pleasant for self-described unprejudiced White subjects).",https://www.semanticscholar.org/paper/10cc2d53ff8349d3432b8f822d58e4ddee3d475e,10.1037/0022-3514.74.6.1464,
Deep Contextualized Word Representations,background;methodology,True,2018,Matthew E. Peters;Mark Neumann;Mohit Iyyer;Matt Gardner;Christopher Clark;Kenton Lee;Luke Zettlemoyer,"We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.",https://www.semanticscholar.org/paper/3febb2bed8865945e7fddc99efd791887bb7e14f,10.18653/v1/N18-1202,1802.05365
Mind the GAP: A Balanced Corpus of Gendered Ambiguous Pronouns,background;methodology,True,2018,Kellie Webster;Marta Recasens;Vera Axelrod;Jason Baldridge,"Coreference resolution is an important task for natural language understanding, and the resolution of ambiguous pronouns a longstanding challenge. Nonetheless, existing corpora do not capture ambiguous pronouns in sufficient volume or diversity to accurately indicate the practical utility of models. Furthermore, we find gender bias in existing corpora and systems favoring masculine entities. To address this, we present and release GAP, a gender-balanced labeled corpus of 8,908 ambiguous pronoun–name pairs sampled to provide diverse coverage of challenges posed by real-world text. We explore a range of baselines that demonstrate the complexity of the challenge, the best achieving just 66.9% F1. We show that syntactic structure and continuous neural models provide promising, complementary cues for approaching the challenge.",https://www.semanticscholar.org/paper/57032c1e327c88a53ab41c17e91bf1406f9ef5c9,10.1162/tacl_a_00240,1810.05201
On Measuring Social Biases in Sentence Encoders,result;background;methodology,True,2019,Chandler May;Alex Wang;Shikha Bordia;Samuel R. Bowman;Rachel Rudinger,"The Word Embedding Association Test shows that GloVe and word2vec word embeddings exhibit human-like implicit biases based on gender, race, and other social constructs (Caliskan et al., 2017). Meanwhile, research on learning reusable text representations has begun to explore sentence-level texts, with some sentence encoders seeing enthusiastic adoption. Accordingly, we extend the Word Embedding Association Test to measure bias in sentence encoders. We then test several sentence encoders, including state-of-the-art methods such as ELMo and BERT, for the social biases studied in prior work and two important biases that are difficult or impossible to test at the word level. We observe mixed results including suspicious patterns of sensitivity that suggest the test’s assumptions may not hold in general. We conclude by proposing directions for future work on measuring bias in sentence encoders.",https://www.semanticscholar.org/paper/5e9c85235210b59a16bdd84b444a904ae271f7e7,10.18653/v1/N19-1063,1903.10561
Algorithms for Non-negative Matrix Factorization,methodology,False,2000,Daniel D. Lee;H. Seung,"Non-negative matrix factorization (NMF) has previously been shown to be a useful decomposition for multivariate data. Two different multiplicative algorithms for NMF are analyzed. They differ only slightly in the multiplicative factor used in the update rules. One algorithm can be shown to minimize the conventional least squares error while the other minimizes the generalized Kullback-Leibler divergence. The monotonic convergence of both algorithms can be proven using an auxiliary function analogous to that used for proving convergence of the Expectation-Maximization algorithm. The algorithms can also be interpreted as diagonally rescaled gradient descent, where the rescaling factor is optimally chosen to ensure convergence.",https://www.semanticscholar.org/paper/6fb07b90b7fd2785ffec0da1069e75c53f7313c2,,
Distributed Representations of Words and Phrases and their Compositionality,background;methodology,True,2013,Tomas Mikolov;Ilya Sutskever;Kai Chen;G. Corrado;J. Dean,"The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. 
 
An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ""Canada"" and ""Air"" cannot be easily combined to obtain ""Air Canada"". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",https://www.semanticscholar.org/paper/87f40e6f3022adbc1f1905e3e506abad05a9964f,,1310.4546
Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word Embeddings,background;methodology,True,2016,Tolga Bolukbasi;Kai-Wei Chang;James Y. Zou;Venkatesh Saligrama;A. Kalai,"The blind application of machine learning runs the risk of amplifying biases present in data. Such a danger is facing us with word embedding, a popular framework to represent text data as vectors which has been used in many machine learning and natural language processing tasks. We show that even word embeddings trained on Google News articles exhibit female/male gender stereotypes to a disturbing extent. This raises concerns because their widespread use, as we describe, often tends to amplify these biases. Geometrically, gender bias is first shown to be captured by a direction in the word embedding. Second, gender neutral words are shown to be linearly separable from gender definition words in the word embedding. Using these properties, we provide a methodology for modifying an embedding to remove gender stereotypes, such as the association between the words receptionist and female, while maintaining desired associations such as between the words queen and female. Using crowd-worker evaluation as well as standard benchmarks, we empirically demonstrate that our algorithms significantly reduce gender bias in embeddings while preserving the its useful properties such as the ability to cluster related concepts and to solve analogy tasks. The resulting embeddings can be used in applications without amplifying gender bias.",https://www.semanticscholar.org/paper/ccf6a69a7f33bcf052aa7def176d3b9de495beb7,,1607.06520
BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,background;methodology,True,2019,Jacob Devlin;Ming-Wei Chang;Kenton Lee;Kristina Toutanova,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).",https://www.semanticscholar.org/paper/df2b0e26d0599ce3e70df8a9da02e51594e0e992,10.18653/v1/N19-1423,1810.04805
Semantics derived automatically from language corpora contain human-like biases,result;background;methodology,True,2016,Aylin Caliskan;J. Bryson;A. Narayanan,"Machines learn what people know implicitly AlphaGo has demonstrated that a machine can learn how to do things that people spend many years of concentrated study learning, and it can rapidly learn how to do them better than any human can. Caliskan et al. now show that machines can learn word associations from written texts and that these associations mirror those learned by humans, as measured by the Implicit Association Test (IAT) (see the Perspective by Greenwald). Why does this matter? Because the IAT has predictive value in uncovering the association between concepts, such as pleasantness and flowers or unpleasantness and insects. It can also tease out attitudes and beliefs—for example, associations between female names and family or male names and career. Such biases may not be expressed explicitly, yet they can prove influential in behavior. Science, this issue p. 183; see also p. 133 Computers can learn which words go together more or less often and can thus mimic human performance on a test of implicit bias. Machine learning is a means to derive artificial intelligence by discovering patterns in existing data. Here, we show that applying machine learning to ordinary human language results in human-like semantic biases. We replicated a spectrum of known biases, as measured by the Implicit Association Test, using a widely used, purely statistical machine-learning model trained on a standard corpus of text from the World Wide Web. Our results indicate that text corpora contain recoverable and accurate imprints of our historic biases, whether morally neutral as toward insects or flowers, problematic as toward race or gender, or even simply veridical, reflecting the status quo distribution of gender with respect to careers or first names. Our methods hold promise for identifying and addressing sources of bias in culture, including technology.",https://www.semanticscholar.org/paper/e211ec0fdaee8bec696475eaffae05af32222b9b,10.1126/science.aal4230,1608.07187
What do you learn from context? Probing for sentence structure in contextualized word representations,methodology,False,2019,Ian Tenney;Patrick Xia;Berlin Chen;Alex Wang;Adam Poliak;R. Thomas McCoy;Najoung Kim;Benjamin Van Durme;Samuel R. Bowman;Dipanjan Das;Ellie Pavlick,"Contextualized representation models such as ELMo (Peters et al., 2018a) and BERT (Devlin et al., 2018) have recently achieved state-of-the-art results on a diverse array of downstream NLP tasks. Building on recent token-level probing work, we introduce a novel edge probing task design and construct a broad suite of sub-sentence tasks derived from the traditional structured NLP pipeline. We probe word-level contextual representations from four recent models and investigate how they encode sentence structure across a range of syntactic, semantic, local, and long-range phenomena. We find that existing models trained on language modeling and translation produce strong representations for syntactic phenomena, but only offer comparably small improvements on semantic tasks over a non-contextual baseline.",https://www.semanticscholar.org/paper/e2587eddd57bc4ba286d91b27c185083f16f40ee,,1905.06316
