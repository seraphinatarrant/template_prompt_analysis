Title,Section,Influential,Year,Authors,Abstract,URL,DOI,Arxiv
Gender Bias in Masked Language Models for Multiple Languages,background;methodology,True,2022,Masahiro Kaneko;Aizhan Imankulova;D. Bollegala;Naoaki Okazaki,"Masked Language Models (MLMs) pretrained by predicting masked tokens on large corpora have been used successfully in natural language processing tasks for a variety of languages. Unfortunately, it was reported that MLMs also learn discriminative biases regarding attributes such as gender and race. Because most studies have focused on MLMs in English, the bias of MLMs in other languages has rarely been investigated. Manual annotation of evaluation data for languages other than English has been challenging due to the cost and difﬁculty in recruiting annotators. More-over, the existing bias evaluation methods require the stereotypical sentence pairs consist-ing of the same context with attribute words (e.g. He/She is a nurse ). We propose Multilingual Bias Evaluation (MBE) score, to evaluate bias in various languages using only English attribute word lists and parallel corpora between the target language and English without requiring manually annotated data. We evaluated MLMs in eight languages using the MBE and conﬁrmed that gender-related biases are encoded in MLMs for all those languages. We manually created datasets for gender bias in Japanese and Russian to evaluate the validity of the MBE. The results show that the bias scores reported by the MBE signiﬁcantly correlates with that computed from the above manually created datasets and the existing English datasets for gender bias.",https://www.semanticscholar.org/paper/0607b299284cb44eaee0aedd95db3c88b00ff944,10.48550/arXiv.2205.00551,2205.00551
You reap what you sow: On the Challenges of Bias Evaluation Under Multilingual Settings,,False,2022,Zeerak Talat1;Aurélie Névéol;Stella Rose Biderman;Miruna Clinciu;Manan Dey;S. Longpre;A. Luccioni;Maraim Masoud;Margaret Mitchell;Dragomir R. Radev;Shanya Sharma;Arjun Subramonian;Jaesung Tae;Samson Tan1;D. Tunuguntla;Oskar van der Wal,"Evaluating bias, fairness, and social impact in monolingual language models is a difficult task. This challenge is further compounded when language modeling occurs in a multilingual context. Considering the implication of evaluation biases for large multilingual language models, we situate the discussion of bias evaluation within a wider context of social scientific research with computational work.We highlight three dimensions of developing multilingual bias evaluation frameworks: (1) increasing transparency through documentation, (2) expanding targets of bias beyond gender, and (3) addressing cultural differences that exist between languages.We further discuss the power dynamics and consequences of training large language models and recommend that researchers remain cognizant of the ramifications of developing such technologies.",https://www.semanticscholar.org/paper/21abb6f22851e5447cd810dd9e70a4b8691cee51,10.18653/v1/2022.bigscience-1.3,
"How Gender Debiasing Affects Internal Model Representations, and Why It Matters",background;methodology,False,2022,Hadas Orgad;Seraphina Goldfarb-Tarrant;Y. Belinkov,"Common studies of gender bias in NLP focus either on extrinsic bias measured by model performance on a downstream task or on intrinsic bias found in models’ internal representations. However, the relationship between extrinsic and intrinsic bias is relatively unknown. In this work, we illuminate this relationship by measuring both quantities together: we debias a model during downstream ﬁne-tuning, which reduces extrinsic bias, and measure the effect on intrinsic bias, which is operationalized as bias extractability with information-theoretic probing. Through experiments on two tasks and multiple bias metrics, we show that our intrinsic bias metric is a better indicator of debiasing than (a contextual adaptation of) the standard WEAT metric, and can also expose cases of superﬁcial debiasing. Our framework provides a comprehensive perspective on bias in NLP models, which can be applied to deploy NLP systems in a more informed manner. 1",https://www.semanticscholar.org/paper/339ab215d8f1f2cb2644346f11df83c5173e7873,10.48550/arXiv.2204.06827,2204.06827
Speciesist Language and Nonhuman Animal Bias in English Masked Language Models,methodology,False,2022,Masashi Takeshita;Rafal Rzepka;K. Araki,"Warning: This paper contains examples of offensive language, including insulting or objectifying expressions. Various existing studies have analyzed what social biases are inherited by NLP models. These biases may directly or indirectly harm people, therefore previous studies have focused only on human attributes. If the social biases in NLP models can be indirectly harmful to humans involved, then the models can also indirectly harm nonhuman animals. However, until recently no research on social biases in NLP regarding nonhumans existed. In this paper1, we analyze biases to nonhuman animals, i.e. speciesist bias, inherent in English Masked Language Models. We analyze this bias using template-based and corpus-extracted sentences which contain speciesist (or non-speciesist) language, to show that these models tend to associate harmful words with nonhuman animals. Our code for reproducing the experiments will be made available on GitHub2.",https://www.semanticscholar.org/paper/40dbdf5dca5d92581573cd7080c4a29f8a8ee9bc,10.48550/arXiv.2203.05140,2203.05140
What do Toothbrushes do in the Kitchen? How Transformers Think our World is Structured,background;methodology,True,2022,Alexander Henlein;Alexander Mehler,"Transformer-based models are now predom-inant in NLP. They outperform approaches based on static models in many respects. This success has in turn prompted research that re-veals a number of biases in the language models generated by transformers. In this paper we utilize this research on biases to investigate to what extent transformer-based language models allow for extracting knowledge about object relations ( X occurs in Y ; X consists of Z ; action A involves using X ). To this end, we compare contextualized models with their static counterparts. We make this comparison dependent on the application of a number of similarity measures and classiﬁers. Our results are threefold: Firstly, we show that the models combined with the different similarity measures differ greatly in terms of the amount of knowledge they allow for extracting. Secondly, our results suggest that similarity measures perform much worse than classiﬁer-based approaches. Thirdly, we show that, surprisingly, static models perform almost as well as contextualized models – in some cases even better.",https://www.semanticscholar.org/paper/682ee9c06f3eff3e3708b4d0419dc85ecf9c6c87,10.48550/arXiv.2204.05673,2204.05673
Loyola eCommons Loyola eCommons Bias Mitigation for Toxicity Detection via Sequential Decisions Bias Mitigation for Toxicity Detection via Sequential Decisions,,False,2022,Lu Cheng;Ahmadreza Mosallanezhad;Yasin N. Silva;Deborah L. Hall;Huan Liu,"Increased social media use has contributed to the greater preva-lence of abusive, rude, and offensive textual comments. Machine learning models have been developed to detect toxic comments online, yet these models tend to show biases against users with marginalized or minority identities (e.g., females and African Amer-icans). Established research in debiasing toxicity classifiers often (1) takes a static or batch approach, assuming that all information is available and then making a one-time decision; and (2) uses a generic strategy to mitigate different biases (e.g., gender and racial biases) that assumes the biases are independent of one another. However, in real scenarios, the input typically arrives as a sequence of comments/words over time instead of all at once. Thus, decisions based on partial information must be made while additional input is arriving. Moreover, social bias is complex by nature. Each type of bias is defined within its unique context, which, consistent with intersectionality theory within the social sciences, might be correlated with the contexts of other forms of bias. In this work, we consider debiasing toxicity detection as a sequential decision-making process where different biases can be interdependent . In particular, we study debiasing toxicity detection with two aims: (1) to examine whether different biases tend to correlate with each other; and (2) to investigate how to jointly mitigate these correlated biases in an interactive manner to minimize the total amount of bias. At the core of our approach is a framework built upon theories of sequential Markov Decision Processes that seeks to maximize the prediction accuracy and minimize the bias measures tailored to individual biases. Evaluations on two benchmark datasets empirically validate the hypothesis to and",https://www.semanticscholar.org/paper/6d33429ad251099068452e4c48901abe036bf60c,,
Hollywood Identity Bias Dataset: A Context Oriented Bias Analysis of Movie Dialogues,,False,2022,Sandhya Singh;Prapti Roy;N. Sahoo;Niteesh Mallela;Himanshu Gupta;P. Bhattacharyya;Milind Savagaonkar;Nidhi;Roshni Ramnani;Anutosh Maitra;Shubhashis Sengupta,"Warning: This paper contains content that may be offensive or upsetting however this cannot be avoided owing to the nature of the work. Movies reﬂect society and also hold power to transform opinions. Social biases and stereotypes present in movies can cause extensive damage due to their reach. These biases are not always found to be the need of storyline but can creep in as the author’s bias. Movie production houses would prefer to ascertain that the bias present in a script is the story’s demand. Today, when deep learning models can give human-level accuracy in multiple tasks, having an AI solution to identify the biases present in the script at the writing stage can help them avoid the inconvenience of stalled release, lawsuits, etc. Since AI solutions are data intensive and there exists no domain speciﬁc data to address the problem of biases in scripts, we introduce a new dataset of movie scripts that are annotated for identity bias. The dataset contains dialogue turns annotated for (i) bias labels for seven categories, viz., gender, race/ethnicity, religion, age, occupation, LGBTQ, and other, which contains biases like body shaming, personality bias, etc. (ii) labels for sensitivity, stereotype, sentiment, emotion, emotion intensity, (iii) all labels annotated with context awareness, (iv) target groups and reason for bias labels and (v) expert-driven group-validation process for high quality annotations. We also report various baseline performances for bias identiﬁcation and category detection on our dataset.",https://www.semanticscholar.org/paper/76b8e36dfd9bee0f67f8f923bc566e5290e6dc47,10.48550/arXiv.2205.15951,2205.15951
Interpretable bias mitigation for textual data: Reducing genderization in patient notes while maintaining classification performance,background;methodology,False,2021,J. Minot;Nicholas Cheney;Marc E. Maier;Danne C. Elbers;C. Danforth;P. Dodds,"Medical systems in general, and patient treatment decisions and outcomes in particular, can be affected by bias based on gender and other demographic elements. As language models are increasingly applied to medicine, there is a growing interest in building algorithmic fairness into processes impacting patient care. Much of the work addressing this question has focused on biases encoded in language models—statistical estimates of the relationships between concepts derived from distant reading of corpora. Building on this work, we investigate how differences in gender-specific word frequency distributions and language models interact with regards to bias. We identify and remove gendered language from two clinical-note datasets and describe a new debiasing procedure using BERT-based gender classifiers. We show minimal degradation in health condition classification tasks for low- to medium-levels of dataset bias removal via data augmentation. Finally, we compare the bias semantically encoded in the language models with the bias empirically observed in health records. This work outlines an interpretable approach for using data augmentation to identify and reduce biases in natural language processing pipelines.",https://www.semanticscholar.org/paper/8994bce4b85a8b4087584661c49f8776f868f7dd,10.1145/3524887,2103.05841
"Upstream Mitigation Is 
 Not
 All You Need: Testing the Bias Transfer Hypothesis in Pre-Trained Language Models",background;methodology,True,2022,Ryan Steed;Swetasudha Panda;Ari Kobren;Michael L. Wick,"A few large, homogenous, pre-trained models undergird many machine learning systems — and often, these models contain harmful stereotypes learned from the internet. We investigate the bias transfer hypothesis: the theory that social biases (such as stereotypes) internalized by large language models during pre-training transfer into harmful task-specific behavior after fine-tuning. For two classification tasks, we find that reducing intrinsic bias with controlled interventions before fine-tuning does little to mitigate the classifier’s discriminatory behavior after fine-tuning. Regression analysis suggests that downstream disparities are better explained by biases in the fine-tuning dataset. Still, pre-training plays a role: simple alterations to co-occurrence rates in the fine-tuning dataset are ineffective when the model has been pre-trained. Our results encourage practitioners to focus more on dataset quality and context-specific harms.",https://www.semanticscholar.org/paper/8d863cafea3493fb033fcdcf9f272a1a4912628b,10.18653/v1/2022.acl-long.247,
RobBERTje: a Distilled Dutch BERT Model,methodology,False,2022,Pieter Delobelle;Thomas Winters;Bettina Berendt,"Pre-trained large-scale language models such as BERT have gained a lot of attention thanks to their outstanding performance on a wide range of natural language tasks. However, due to their large number of parameters, they are resource-intensive both to deploy and to ﬁne-tune. Researchers have created several methods for distilling language models into smaller ones to increase eﬃciency, with a small performance trade-oﬀ. In this paper, we create several diﬀerent distilled versions of the state-of-the-art Dutch RobBERT model and call them RobBERTje. The distillations diﬀer in their distillation corpus, namely whether or not they are shuﬄed and whether they are merged with subsequent sentences. We found that the performance of the models using the shuﬄed versus non-shuﬄed datasets is similar for most tasks and that randomly merging subsequent sentences in a corpus creates models that train faster and perform better on tasks with long sequences. Upon comparing distillation architectures, we found that the larger DistilBERT architecture worked signiﬁcantly better than the Bort hyperparametrization. Interestingly, we also found that the distilled models exhibit less gender-stereotypical bias than its teacher model. Since smaller architectures decrease the time to ﬁne-tune, these models allow for more eﬃcient training and more lightweight deployment of many Dutch downstream language tasks.",https://www.semanticscholar.org/paper/98e391f1905043deec2ab2123f06ba16ae72dbb1,10.48550/arXiv.2204.13511,2204.13511
Applying the Stereotype Content Model to assess disability bias in popular pre-trained NLP models underlying AI-based assistive technologies,,False,2022,Brienna Herold;James C. Waller;R. Kushalnagar,"Stereotypes are a positive or negative, generalized, and often widely shared belief about the attributes of certain groups of people, such as people with sensory disabilities. If stereotypes manifest in assistive technologies used by deaf or blind people, they can harm the user in a number of ways, especially considering the vulnerable nature of the target population. AI models underlying assistive technologies have been shown to contain biased stereotypes, including racial, gender, and disability biases. We build on this work to present a psychology-based stereotype assessment of the representation of disability, deafness, and blindness in BERT using the Stereotype Content Model. We show that BERT contains disability bias, and that this bias differs along established stereotype dimensions.",https://www.semanticscholar.org/paper/a50ee4f847eef701c514514d63d0cb11be9facc3,10.18653/v1/2022.slpat-1.8,
Sense Embeddings are also Biased – Evaluating Social Biases in Static and Contextualised Sense Embeddings,background;methodology,False,2022,Yi Zhou;Masahiro Kaneko;D. Bollegala,"Sense embedding learning methods learn different embeddings for the different senses of an ambiguous word. One sense of an ambiguous word might be socially biased while its other senses remain unbiased. In comparison to the numerous prior work evaluating the social biases in pretrained word embeddings, the biases in sense embeddings have been relatively understudied. We create a benchmark dataset for evaluating the social biases in sense embeddings and propose novel sense-specific bias evaluation measures. We conduct an extensive evaluation of multiple static and contextualised sense embeddings for various types of social biases using the proposed measures. Our experimental results show that even in cases where no biases are found at word-level, there still exist worrying levels of social biases at sense-level, which are often ignored by the word-level bias evaluation measures.",https://www.semanticscholar.org/paper/a69427573665e3bd376827953353cf0f6871bc46,10.48550/arXiv.2203.07523,2203.07523
"Theories of ""Gender"" in NLP Bias Research",,False,2022,Hannah Devinney;Jenny Björklund;Henrik Björklund,"The rise of concern around Natural Language Processing (NLP) technologies containing and perpetuating social biases has led to a rich and rapidly growing area of research. Gender bias is one of the central biases being analyzed, but to date there is no comprehensive analysis of how “gender” is theorized in the field. We survey nearly 200 articles concerning gender bias in NLP to discover how the field conceptualizes gender both explicitly (e.g. through definitions of terms) and implicitly (e.g. through how gender is operationalized in practice). In order to get a better idea of emerging trajectories of thought, we split these articles into two sections by time. We find that the majority of the articles do not make their theorization of gender explicit, even if they clearly define “bias.” Almost none use a model of gender that is intersectional or inclusive of nonbinary genders; and many conflate sex characteristics, social gender, and linguistic gender in ways that disregard the existence and experience of trans, nonbinary, and intersex people. There is an increase between the two time-sections in statements acknowledging that gender is a complicated reality, however, very few articles manage to put this acknowledgment into practice. In addition to analyzing these findings, we provide specific recommendations to facilitate interdisciplinary work, and to incorporate theory and methodology from Gender Studies. Our hope is that this will produce more inclusive gender bias research in NLP.",https://www.semanticscholar.org/paper/b2563d102456d5140ecb4111e7f08481f720d9a4,10.48550/arXiv.2205.02526,2205.02526
From Examples to Rules: Neural Guided Rule Synthesis for Information Extraction,,False,2022,Robert Vacareanu;M. A. Valenzuela-Escarcega;G. Barbosa;Rebecca Sharp;M. Surdeanu,"While deep learning approaches to information extraction have had many successes, they can be difficult to augment or maintain as needs shift. Rule-based methods, on the other hand, can be more easily modified. However, crafting rules requires expertise in linguistics and the domain of interest, making it infeasible for most users. Here we attempt to combine the advantages of these two directions while mitigating their drawbacks. We adapt recent advances from the adjacent field of program synthesis to information extraction, synthesizing rules from provided examples. We use a transformer-based architecture to guide an enumerative search, and show that this reduces the number of steps that need to be explored before a rule is found. Further, we show that without training the synthesis algorithm on the specific domain, our synthesized rules achieve state-of-the-art performance on the 1-shot scenario of a task that focuses on few-shot learning for relation classification, and competitive performance in the 5-shot scenario.",https://www.semanticscholar.org/paper/bc14342aa2bc6264a0d82c0c3cc7f809cc5e57c1,,2202.00475
An Empirical Survey of the Effectiveness of Debiasing Techniques for Pre-trained Language Models,result;background,False,2021,Nicholas Meade;Elinor Poole-Dayan;Siva Reddy,"Recent work has shown pre-trained language models capture social biases from the large amounts of text they are trained on. This has attracted attention to developing techniques that mitigate such biases. In this work, we perform an empirical survey of five recently proposed bias mitigation techniques: Counterfactual Data Augmentation (CDA), Dropout, Iterative Nullspace Projection, Self-Debias, and SentenceDebias. We quantify the effectiveness of each technique using three intrinsic bias benchmarks while also measuring the impact of these techniques on a model’s language modeling ability, as well as its performance on downstream NLU tasks. We experimentally find that: (1) Self-Debias is the strongest debiasing technique, obtaining improved scores on all bias benchmarks; (2) Current debiasing techniques perform less consistently when mitigating non-gender biases; And (3) improvements on bias benchmarks such as StereoSet and CrowS-Pairs by using debiasing strategies are often accompanied by a decrease in language modeling ability, making it difficult to determine whether the bias mitigation was effective.",https://www.semanticscholar.org/paper/de6807676d8171472ed6cf421c4e4ed3cbb47699,10.18653/v1/2022.acl-long.132,2110.08527
Using Cross-Loss Influence Functions to Explain Deep Network Representations,,False,2020,Andrew Silva;Rohit Chopra;M. Gombolay,"As machine learning is increasingly deployed in the real world, it is ever more vital that we understand the decision-criteria of the models we train. Recently, researchers have shown that influence functions, a statistical measure of sample impact, may be extended to approximate the effects of training samples on classification accuracy for deep neural networks. However, prior work only applies to supervised learning setups where training and testing share an objective function. Despite the rise in unsupervised learning, self-supervised learning, and model pre-training, there are currently no suitable technologies for estimating influence of deep networks that do not train and test on the same objective. To overcome this limitation, we provide the first theoretical and empirical demonstration that influence functions can be extended to handle mismatched training and testing settings. Our result enables us to compute the influence of unsupervised and self-supervised training examples with respect to a supervised test objective. We demonstrate this technique on a synthetic dataset as well as two Skip-gram language model examples to examine cluster membership and sources of unwanted bias.",https://www.semanticscholar.org/paper/e0cc14403b3d57572e837f9f3b1cca87d70e2bcd,,2012.01685
SMU Multi-Modal Classification Using Images and Text Multi-Modal Classification Using Images and Text Multi-Modal Classification Using Images and Text,,False,2022,S. Miller;Justin Howard;Paul Adams;Mel Schwan;R. Slater,". This paper proposes a method for the integration of natural language understanding in image classification to improve classification accuracy by making use of associated metadata. Traditionally, only image features have been used in the classification process; however, metadata accompanies images from many sources. This study implemented a multi-modal image classification model that combines convolutional methods with natural language understanding of descriptions, titles, and tags to improve image classification. The novelty of this approach was to learn from additional external features associated with the images using natural language understanding with transfer learning. It was found that the combination of ResNet-50 image feature extraction and Universal Sentence Encoder embeddings yielded a Top 5 error rate of 73.05% and Top 1 error rate of 54.65%, which is an improvement of 1.56% on benchmark results. This suggests external text features can be used to aid image classification when they are available.",https://www.semanticscholar.org/paper/e349f31607730cf04e254d4e7d8987d08d5d126b,,
Measuring Gender Bias in Contextualized Embeddings,result;methodology,True,2022,Styliani Katsarou;Borja Rodríguez-Gálvez;Jesse Shanahan,"Transformer models are now increasingly being used in real-world applications. Indiscriminately using these models as automated tools may propagate biases in ways we do not realize. To responsibly direct actions that will combat this problem, it is of crucial importance that we detect and quantify these biases. Robust methods have been developed to measure bias in non-contextualized embeddings. Nevertheless, these methods fail to apply to contextualized embeddings due to their mutable nature. Our study focuses on the detection and measurement of stereotypical biases associated with gender in the embeddings of T5 and mT5. We quantify bias by measuring the gender polarity of T5’s word embeddings for various professions. To measure gender polarity, we use a stable gender direction that we detect in the model’s embedding space. We also measure gender bias with respect to a specific downstream task and compare Swedish with English, as well as various sizes of the T5 model and its multilingual variant. The insights from our exploration indicate that the use of a stable gender direction, even in a Transformer’s mutable embedding space, can be a robust method to measure bias. We show that higher status professions are associated more with the male gender than the female gender. In addition, our method suggests that the Swedish language carries less bias associated with gender than English, and the higher manifestation of gender bias is associated with the use of larger language models.",https://www.semanticscholar.org/paper/effe8668b9ac4f22c8bbc421ba87df102cb807b0,10.3390/cmsf2022003003,
A Survey on Gender Bias in Natural Language Processing,,False,2021,Karolina Stańczak;Isabelle Augenstein,"Language can be used as a means of reproducing and enforcing harmful stereotypes and biases and has been analysed as such in numerous research. In this paper, we present a survey of 304 papers on gender bias in natural language processing. We analyse definitions of gender and its categories within social sciences and connect them to formal definitions of gender bias in NLP research. We survey lexica and datasets applied in research on gender bias and then compare and contrast approaches to detecting and mitigating gender bias. We find that research on gender bias suffers from four core limitations. 1) Most research treats gender as a binary variable neglecting its fluidity and continuity. 2) Most of the work has been conducted in monolingual setups for English or other high-resource languages. 3) Despite a myriad of papers on gender bias in NLP methods, we find that most of the newly developed algorithms do not test their models for bias and disregard possible ethical considerations of their work. 4) Finally, methodologies developed in this line of research are fundamentally flawed covering very limited definitions of gender bias and lacking evaluation baselines and pipelines. We see overcoming these limitations as a necessary development in future research.",https://www.semanticscholar.org/paper/04ec406caebff60e226695c921f0af1b29162c5f,,2112.14168
"Hi, my name is Martha: Using names to measure and mitigate bias in generative dialogue models",,False,2021,Eric Michael Smith;Adina Williams,"All AI models are susceptible to learning biases in data that they are trained on. For generative dialogue models, being trained on real human conversations containing unbalanced gender and race/ethnicity references can lead to models that display learned biases, which we define here broadly as any measurable differences in the distributions of words or semantic content of conversations based on demographic groups. We measure the strength of such biases by producing artificial conversations between two copies of a dialogue model, conditioning one conversational partner to state a name commonly associated with a certain gender and/or race/ethnicity. We find that larger capacity models tend to exhibit more gender bias and greater stereotyping of occupations by gender. We show that several methods of tuning these dialogue models, specifically name scrambling, controlled generation, and unlikelihood training, are effective in reducing bias in conversation, including on a downstream conversational task. Name scrambling is also effective in lowering differences in token usage across conversations where partners have names associated with different genders or races/ethnicities.",https://www.semanticscholar.org/paper/063183d95a249d94c95d12e7e9462e0aa84b6d85,,2109.03300
Quantifying Social Biases in NLP: A Generalization and Empirical Comparison of Extrinsic Fairness Metrics,background;methodology,False,2021,Paula Czarnowska;Yogarshi Vyas;Kashif Shah,"Abstract Measuring bias is key for better understanding and addressing unfairness in NLP/ML models. This is often done via fairness metrics, which quantify the differences in a model’s behaviour across a range of demographic groups. In this work, we shed more light on the differences and similarities between the fairness metrics used in NLP. First, we unify a broad range of existing metrics under three generalized fairness metrics, revealing the connections between them. Next, we carry out an extensive empirical comparison of existing metrics and demonstrate that the observed differences in bias measurement can be systematically explained via differences in parameter choices for our generalized metrics.",https://www.semanticscholar.org/paper/10aa2be24951e6de76b630482a645d79354c4cde,10.1162/tacl_a_00425,2106.14574
Towards Understanding and Mitigating Social Biases in Language Models,background;methodology,False,2021,Paul Pu Liang;Chiyu Wu;Louis-Philippe Morency;R. Salakhutdinov,"Warning: this paper contains model outputs that may be offensive or upsetting. As machine learning methods are deployed in realworld settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for highfidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",https://www.semanticscholar.org/paper/114aa720872462b0ca1b97bfdec0ebd56c36fd0a,,2106.13219
Sustainable Modular Debiasing of Language Models,background;methodology,False,2021,Anne Lauscher;Tobias Lüken;Goran Glavas,"Unfair stereotypical biases (e.g., gender, racial, or religious biases) encoded in modern pretrained language models (PLMs) have nega-tive ethical implications for widespread adop-tion of state-of-the-art language technology. To remedy for this, a wide range of debiasing techniques have recently been introduced to remove such stereotypical biases from PLMs. Existing debiasing methods, however, directly modify all of the PLMs parameters, which – besides being computationally expensive – comes with the inherent risk of (catastrophic) forgetting of useful language knowledge acquired in pretraining. In this work, we propose a more sustainable modular debiasing approach based on dedicated debiasing adapters , dubbed A DELE . Concretely, we (1) inject adapter modules into the original PLM layers and (2) update only the adapters (i.e., we keep the original PLM parameters frozen) via language modeling training on a counterfactually augmented corpus. We showcase A DELE in gender debiasing of BERT: our extensive evaluation, encompassing three intrinsic and two extrinsic bias measures, renders A DELE very effective in bias mitigation. We further show that – due to its modular nature – A DELE , cou-pled with task adapters, retains fairness even after large-scale downstream training. Finally, by means of multilingual BERT, we successfully transfer A DELE to six target languages.",https://www.semanticscholar.org/paper/130ab5c480860e330b65280a3410f17bb2d50fe1,10.18653/v1/2021.findings-emnlp.411,2109.03646
Mitigating Language-Dependent Ethnic Bias in BERT,background;methodology,True,2021,Jaimeen Ahn;Alice H. Oh,"In this paper, we study ethnic bias and how it varies across languages by analyzing and mitigating ethnic bias in monolingual BERT for English, German, Spanish, Korean, Turkish, and Chinese. To observe and quantify ethnic bias, we develop a novel metric called Categorical Bias score. Then we propose two methods for mitigation; first using a multilingual model, and second using contextual word alignment of two monolingual models. We compare our proposed methods with monolingual BERT and show that these methods effectively alleviate the ethnic bias. Which of the two methods works better depends on the amount of NLP resources available for that language. We additionally experiment with Arabic and Greek to verify that our proposed methods work for a wider variety of languages.",https://www.semanticscholar.org/paper/1aa1d6b29ad6fcef78d1eefacb2a7fd75e68c2c0,10.18653/v1/2021.emnlp-main.42,2109.05704
Worst of Both Worlds: Biases Compound in Pre-trained Vision-and-Language Models,background;methodology,True,2021,Tejas Srinivasan;Yonatan Bisk,"Numerous works have analyzed biases in vision and pre-trained language models individ-ually - however, less attention has been paid to how these biases interact in multimodal set-tings. This work extends text-based bias analysis methods to investigate multimodal language models, and analyzes intra- and inter-modality associations and biases learned by these models. Speciﬁcally, we demonstrate that VL-BERT (Su et al., 2020) exhibits gender biases, often preferring to reinforce a stereotype over faithfully describing the visual scene. We demonstrate these ﬁndings on a controlled case-study and extend them for a larger set of stereotypically gendered entities.",https://www.semanticscholar.org/paper/1e83a4a3cc65229403a5f90229007af957c12602,,2104.08666
Biomedical Question Answering: A Comprehensive Review,result,False,2021,Qiao Jin;Zheng Yuan;Guangzhi Xiong;Qian Yu;Chuanqi Tan;Mosha Chen;Songfang Huang;Xiaozhong Liu;Sheng Yu,"Question Answering (QA) is a benchmark Natural Language Processing (NLP) task where models predict the answer for a given question using related documents, images, knowledge bases and question-answer pairs. Automatic QA has been successfully applied in various domains like search engines and chatbots. However, for specific domains like biomedicine, QA systems are still rarely used in real-life settings. Biomedical QA (BQA), as an emerging QA task, enables innovative applications to effectively perceive, access and understand complex biomedical knowledge. In this work, we provide a critical review of recent efforts in BQA. We comprehensively investigate prior BQA approaches, which are classified into 6 major methodologies (open-domain, knowledge base, information retrieval, machine reading comprehension, question entailment and visual QA), 4 topics of contents (scientific, clinical, consumer health and examination) and 5 types of formats (yes/no, extraction, generation, multi-choice and retrieval). In the end, we highlight several key challenges of BQA and explore potential directions for future works.",https://www.semanticscholar.org/paper/1f96539c083d60fa83f7548bc6996cdede1026ee,,
Detecting Cross-Geographic Biases in Toxicity Modeling on Social Media,result;background,False,2021,Sayan Ghosh;Dylan Baker;David Jurgens;Vinodkumar Prabhakaran,"Online social media platforms increasingly rely on Natural Language Processing (NLP) techniques to detect abusive content at scale in order to mitigate the harms it causes to their users. However, these techniques suffer from various sampling and association biases present in training data, often resulting in sub-par performance on content relevant to marginalized groups, potentially furthering disproportionate harms towards them. Studies on such biases so far have focused on only a handful of axes of disparities and subgroups that have annotations/lexicons available. Consequently, biases concerning non-Western contexts are largely ignored in the literature. In this paper, we introduce a weakly supervised method to robustly detect lexical biases in broader geo-cultural contexts. Through a case study on a publicly available toxicity detection model, we demonstrate that our method identifies salient groups of cross-geographic errors, and, in a follow up, demonstrate that these groupings reflect human judgments of offensive and inoffensive language in those geographic contexts. We also conduct analysis of a model trained on a dataset with ground truth labels to better understand these biases, and present preliminary mitigation experiments.",https://www.semanticscholar.org/paper/2b314520e1f265698da12707c1cc644c81239144,10.18653/v1/2021.wnut-1.35,2104.06999
Towards a Comprehensive Understanding and Accurate Evaluation of Societal Biases in Pre-Trained Transformers,result;background;methodology,True,2021,Andrew Silva;Pradyumna Tambwekar;M. Gombolay,"The ease of access to pre-trained transformers has enabled developers to leverage large-scale language models to build exciting applications for their users. While such pre-trained models offer convenient starting points for researchers and developers, there is little consideration for the societal biases captured within these model risking perpetuation of racial, gender, and other harmful biases when these models are deployed at scale. In this paper, we investigate gender and racial bias across ubiquitous pre-trained language models, including GPT-2, XLNet, BERT, RoBERTa, ALBERT and DistilBERT. We evaluate bias within pre-trained transformers using three metrics: WEAT, sequence likelihood, and pronoun ranking. We conclude with an experiment demonstrating the ineffectiveness of word-embedding techniques, such as WEAT, signaling the need for more robust bias testing in transformers.",https://www.semanticscholar.org/paper/2cc0e605470d3ac20aad82c73560b888ecc449cd,10.18653/V1/2021.NAACL-MAIN.189,
Confronting Abusive Language Online: A Survey from the Ethical and Human Rights Perspective,,False,2020,Svetlana Kiritchenko;I. Nejadgholi;Kathleen C. Fraser,"The pervasiveness of abusive content on the internet can lead to severe psychological and physical harm. Significant effort in Natural Language Processing (NLP) research has been devoted to addressing this problem through abusive content detection and related sub-areas, such as the detection of hate speech, toxicity, cyberbullying, etc. Although current technologies achieve high classification performance in research studies, it has been observed that the real-life application of this technology can cause unintended harms, such as the silencing of under-represented groups. We review a large body of NLP research on automatic abuse detection with a new focus on ethical challenges, organized around eight established ethical principles: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. In many cases, these principles relate not only to situational ethical codes, which may be context-dependent, but are in fact connected to universal human rights, such as the right to privacy, freedom from discrimination, and freedom of expression. We highlight the need to examine the broad social impacts of this technology, and to bring ethical and human rights considerations to every stage of the application life-cycle, from task formulation and dataset design, to model training and evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including ‘nudging’, ‘quarantining’, value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.evaluation, to application deployment. Guided by these principles, we identify several opportunities for rights-respecting, socio-technical solutions to detect and confront online abuse, including 'nudging', 'quarantining', value sensitive design, counter-narratives, style transfer, and AI-driven public education applications.",https://www.semanticscholar.org/paper/2cf2d1491f72f198ae9990971cf2846e9fe51141,10.1613/jair.1.12590,2012.12305
Evaluating Gender Bias in Hindi-English Machine Translation,methodology,False,2021,Gauri Gupta;Krithika Ramesh;Sanjay Singh,"With language models being deployed increasingly in the real world, it is essential to address the issue of the fairness of their outputs. The word embedding representations of these language models often implicitly draw unwanted associations that form a social bias within the model. The nature of gendered languages like Hindi, poses an additional problem to the quantification and mitigation of bias, owing to the change in the form of the words in the sentence, based on the gender of the subject. Additionally, there is sparse work done in the realm of measuring and debiasing systems for Indic languages. In our work, we attempt to evaluate and quantify the gender bias within a Hindi-English machine translation system. We implement a modified version of the existing TGBI metric based on the grammatical considerations for Hindi. We also compare and contrast the resulting bias measurements across multiple metrics for pre-trained embeddings and the ones learned by our machine translation model.",https://www.semanticscholar.org/paper/2ff522a22d744938bf5150a022904166d4dd45f8,10.18653/v1/2021.gebnlp-1.3,2106.08680
Measuring Biases of Word Embeddings: What Similarity Measures and Descriptive Statistics to Use?,background;methodology,True,2021,Hossein Azarpanah;Mohsen Farhadloo,"Word embeddings are widely used in Natural Language Processing (NLP) for a vast range of applications. However, it has been consistently proven that these embeddings reflect the same human biases that exist in the data used to train them. Most of the introduced bias indicators to reveal word embeddings’ bias are average-based indicators based on the cosine similarity measure. In this study, we examine the impacts of different similarity measures as well as other descriptive techniques than averaging in measuring the biases of contextual and non-contextual word embeddings. We show that the extent of revealed biases in word embeddings depends on the descriptive statistics and similarity measures used to measure the bias. We found that over the ten categories of word embedding association tests, Mahalanobis distance reveals the smallest bias, and Euclidean distance reveals the largest bias in word embeddings. In addition, the contextual models reveal less severe biases than the non-contextual word embedding models.",https://www.semanticscholar.org/paper/300c10da7c3d5920e652d3bef2493baaa228f56a,10.18653/V1/2021.TRUSTNLP-1.2,
Investigating Gender Bias in BERT,,False,2020,Rishabh Bhardwaj;Navonil Majumder;Soujanya Poria,,https://www.semanticscholar.org/paper/36d7998ec3e462e3907c6d3c1d13984234f7e8cc,10.1007/S12559-021-09881-2,2009.05021
Diachronic Analysis of German Parliamentary Proceedings: Ideological Shifts through the Lens of Political Biases,background;methodology,False,2021,Tobias Walter;Celina Kirschner;Steffen Eger;Goran Glavavs;Anne Lauscher;Simone Paolo Ponzetto,"We analyze bias in historical corpora as encoded in diachronic distributional semantic models by focusing on two specific forms of bias, namely a political (i.e., anti-communism) and racist (i.e., antisemitism) one. For this, we use a new corpus of German parliamentary proceedings, Deuparl, spanning the period 1867–2020. We complement this analysis of historical biases in diachronic word embeddings with a novel measure of bias on the basis of term co-occurrences and graph-based label propagation. The results of our bias measurements align with commonly perceived historical trends of antisemitic and anticommunist biases in German politics in different time periods, thus indicating the viability of analyzing historical bias trends using semantic spaces induced from historical corpora.",https://www.semanticscholar.org/paper/41ac23efdd1b93d984c969f534cf3bde223f18fd,10.1109/JCDL52503.2021.00017,2108.06295
Unmasking the Mask - Evaluating Social Biases in Masked Language Models,result;background;methodology,True,2021,Masahiro Kaneko;D. Bollegala,"Masked Language Models (MLMs) have shown superior performances in numerous downstream NLP tasks when used as text encoders. Unfortunately, MLMs also demonstrate significantly worrying levels of social biases. We show that the previously proposed evaluation metrics for quantifying the social biases in MLMs are problematic due to following reasons: (1) prediction accuracy of the masked tokens itself tend to be low in some MLMs, which raises questions regarding the reliability of the evaluation metrics that use the (pseudo) likelihood of the predicted tokens, and (2) the correlation between the prediction accuracy of the mask and the performance in downstream NLP tasks is not taken into consideration, and (3) high frequency words in the training data are masked more often, introducing noise due to this selection bias in the test cases. To overcome the abovementioned disfluencies, we propose All Unmasked Likelihood (AUL), a bias evaluation measure that predicts all tokens in a test case given the MLM embedding of the unmasked input. We find that AUL accurately detects different types of biases in MLMs. We also propose AUL with Attention weights (AULA) to evaluate tokens based on their importance in a sentence. However, unlike AUL and AULA, previously proposed bias evaluation measures for MLMs systematically overestimate the measured biases, and are heavily influenced by the unmasked tokens in the context.",https://www.semanticscholar.org/paper/437727b6c00a5eb4944600091f66f41626d1002d,,2104.07496
Discovering and Categorising Language Biases in Reddit,methodology,False,2020,Xavier Ferrer Aran;T. Nuenen;J. Such;N. Criado,"We present a data-driven approach using word embeddings to discover and categorise language biases on the discussion platform Reddit. As spaces for isolated user communities, platforms such as Reddit are increasingly connected to issues of racism, sexism and other forms of discrimination. Hence, there is a need to monitor the language of these groups. One of the most promising AI approaches to trace linguistic biases in large textual datasets involves word embeddings, which transform text into high-dimensional dense vectors and capture semantic relations between words. Yet, previous studies require predefined sets of potential biases to study, e.g., whether gender is more or less associated with particular types of jobs. This makes these approaches unfit to deal with smaller and community-centric datasets such as those on Reddit, which contain smaller vocabularies and slang, as well as biases that may be particular to that community. This paper proposes a data-driven approach to automatically discover language biases encoded in the vocabulary of online discourse communities on Reddit. In our approach, protected attributes are connected to evaluative words found in the data, which are then categorised through a semantic analysis system. We verify the effectiveness of our method by comparing the biases we discover in the Google News dataset with those found in previous literature. We then successfully discover gender bias, religion bias, and ethnic bias in different Reddit communities. We conclude by discussing potential application scenarios and limitations of this data-driven bias discovery method.",https://www.semanticscholar.org/paper/43b184a9912ecb6de0454892c68c82200f77f234,,2008.02754
NxMTransformer: Semi-Structured Sparsification for Natural Language Understanding via ADMM,,False,2021,Connor Holmes;Minjia Zhang;Yuxiong He;Bo Wu,"Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained Transformer networks. However, these models often contain hundreds of millions or even billions of parameters, bringing challenges to online deployment due to latency constraints. Recently, hardware manufacturers have introduced dedicated hardware for NxM sparsity to provide the flexibility of unstructured pruning with the runtime efficiency of structured approaches. NxM sparsity permits arbitrarily selecting M parameters to retain from a contiguous group of N in the dense representation. However, due to the extremely high complexity of pre-trained models, the standard sparse fine-tuning techniques often fail to generalize well on downstream tasks, which have limited data resources. To address such an issue in a principled manner, we introduce a new learning framework, called NxMTransformer, to induce NxM semi-structured sparsity on pretrained language models for natural language understanding to obtain better performance. In particular, we propose to formulate the NxM sparsity as a constrained optimization problem and use Alternating Direction Method of Multipliers (ADMM) to optimize the downstream tasks while taking the underlying hardware constraints into consideration. ADMM decomposes the NxM sparsification problem into two sub-problems that can be solved sequentially, generating sparsified Transformer networks that achieve high accuracy while being able to effectively execute on newly released hardware. We apply our approach to a wide range of NLP tasks, and our proposed method is able to achieve 1.7 points higher accuracy in GLUE score than current best practices. Moreover, we perform detailed analysis on our approach and shed light on how ADMM affects fine-tuning accuracy for downstream tasks. Finally, we illustrate how NxMTransformer achieves additional performance improvement with knowledge distillation based methods.",https://www.semanticscholar.org/paper/511e2fe08d8ce170736b59d386a3301a6d6e57b0,,2110.15766
Measuring and Improving BERT’s Mathematical Abilities by Predicting the Order of Reasoning.,result;background,False,2021,Piotr Pikekos;Henryk Michalewski;Mateusz Malinowski,"Imagine you are in a supermarket. You have two bananas in your basket and want to buy four apples. How many fruits do you have in total? This seemingly straightforward question can be challenging for data-driven language models, even if trained at scale. However, we would expect such generic language models to possess some mathematical abilities in addition to typical linguistic competence. Towards this goal, we investigate if a commonly used language model, BERT, possesses such mathematical abilities and, if so, to what degree. For that, we fine-tune BERT on a popular dataset for word math problems, AQuA-RAT, and conduct several tests to understand learned representations better. Since we teach models trained on natural language to do formal mathematics, we hypothesize that such models would benefit from training on semi-formal steps that explain how math results are derived. To better accommodate such training, we also propose new pretext tasks for learning mathematical rules. We call them (Neighbor) Reasoning Order Prediction (ROP or NROP). With this new model, we achieve significantly better outcomes than data-driven baselines and even on-par with more tailored models.",https://www.semanticscholar.org/paper/57a1258571a21817d89197dc84c986861fb6e580,10.18653/v1/2021.acl-short.49,2106.03921
Debiasing Pre-trained Contextualised Embeddings,background;methodology,False,2021,Masahiro Kaneko;Danushka Bollegala,"In comparison to the numerous debiasing methods proposed for the static non-contextualised word embeddings, the discriminative biases in contextualised embeddings have received relatively little attention. We propose a fine-tuning method that can be applied at token- or sentence-levels to debias pre-trained contextualised embeddings. Our proposed method can be applied to any pre-trained contextualised embedding model, without requiring to retrain those models. Using gender bias as an illustrative example, we then conduct a systematic study using several state-of-the-art (SoTA) contextualised representations on multiple benchmark datasets to evaluate the level of biases encoded in different contextualised embeddings before and after debiasing using the proposed method. We find that applying token-level debiasing for all tokens and across all layers of a contextualised embedding model produces the best performance. Interestingly, we observe that there is a trade-off between creating an accurate vs. unbiased contextualised embedding model, and different contextualised embedding models respond differently to this trade-off.",https://www.semanticscholar.org/paper/61ca0040d81c5ed71d3f9b9e5f7b528275048440,10.18653/v1/2021.eacl-main.107,2101.09523
Measuring Fairness with Biased Rulers: A Survey on Quantifying Biases in Pretrained Language Models,background;methodology,True,2021,Pieter Delobelle;Ewoenam Kwaku Tokpo;T. Calders;Bettina Berendt,"An increasing awareness of biased patterns in natural language processing resources, like BERT, has motivated many metrics to quantify ‘bias’ and ‘fairness’. But comparing the results of different metrics and the works that evaluate with such metrics remains difficult, if not outright impossible. We survey the existing literature on fairness metrics for pretrained language models and experimentally evaluate compatibility, including both biases in language models as in their downstream tasks. We do this by a mixture of traditional literature survey and correlation analysis, as well as by running empirical evaluations. We find that many metrics are not compatible and highly depend on (i) templates, (ii) attribute and target seeds and (iii) the choice of embeddings. These results indicate that fairness or bias evaluation remains challenging for contextualized language models, if not at least highly subjective. To improve future comparisons and fairness evaluations, we recommend avoiding embedding-based metrics and focusing on fairness evaluations in downstream tasks.",https://www.semanticscholar.org/paper/67ad491b16bf77e9a54a8b8b1dc23dadc5545467,,2112.07447
An Empirical Investigation of Learning from Biased Toxicity Labels,,False,2021,Neel Nanda;Jonathan Uesato;Sven Gowal,"Collecting annotations from human raters often results in a trade-off between the quantity of labels one wishes to gather and the quality of these labels. As such, it is often only possible to gather a small amount of high-quality labels. In this paper, we study how different training strategies can leverage a small dataset of human-annotated labels and a large but noisy dataset of synthetically generated labels (which exhibit bias against identity groups) for predicting toxicity of online comments. We evaluate the accuracy and fairness properties of these approaches, and trade-offs between the two. While we find that initial training on all of the data and fine-tuning on clean data produces models with the highest AUC, we find that no single strategy performs best across all fairness metrics.",https://www.semanticscholar.org/paper/6ccac815519e8ad4da6c25bcd61e5b9f7aec547f,,2110.01577
Investigating Cross-Linguistic Gender Bias in Hindi-English Across Domains,methodology,False,2021,Somya Khosla,.............................................................................................................................. 2 TABLE OF CONTENTS ........................................................................................................... 3 CHAPTER 1 ............................................................................................................................... 5 INTRODUCTION ...................................................................................................................... 5 1.1 Background of the Study ......................................................................................... 5 1.2 Problem Statement ................................................................................................... 5 1.3 Aim and Objectives .................................................................................................. 6 1.4 Scope of the Study .................................................................................................... 6 1.5 Significance of the Study ......................................................................................... 6 1.6 Structure of the Study.............................................................................................. 7 CHAPTER 2 ............................................................................................................................... 8 LITERATURE REVIEW ........................................................................................................... 8 2.,https://www.semanticscholar.org/paper/6fb5dc674bf0013ad5e269b3905c0f4253c3890b,,2111.11159
Sexism in the Judiciary: The Importance of Bias Definition in NLP and In Our Courts,,False,2021,Noa Baker Gillis,"We analyze 6.7 million case law documents to determine the presence of gender bias within our judicial system. We find that current bias detection methods in NLP are insufficient to determine gender bias in our case law database and propose an alternative approach. We show that existing algorithms’ inconsistent results are consequences of prior research’s inconsistent definitions of biases themselves. Bias detection algorithms rely on groups of words to represent bias (e.g., ‘salary,’ ‘job,’ and ‘boss’ to represent employment as a potentially biased theme against women in text). However, the methods to build these groups of words have several weaknesses, primarily that the word lists are based on the researchers’ own intuitions. We suggest two new methods of automating the creation of word lists to represent biases. We find that our methods outperform current NLP bias detection methods. Our research improves the capabilities of NLP technology to detect bias and highlights gender biases present in influential case law. In order to test our NLP bias detection method’s performance, we regress our results of bias in case law against U.S census data of women’s participation in the workforce in the last 100 years.",https://www.semanticscholar.org/paper/7e069b99a73efac0b063d068528de10007998fd4,10.18653/v1/2021.gebnlp-1.6,
Unpacking the Interdependent Systems of Discrimination: Ableist Bias in NLP Systems through an Intersectional Lens,methodology,False,2021,Saad Hassan;Matt Huenerfauth;Cecilia Ovesdotter Alm,"Much of the world’s population experiences some form of disability during their lifetime. Caution must be exercised while designing natural language processing (NLP) systems to prevent systems from inadvertently perpetuat-ing ableist bias against people with disabilities, i.e., prejudice that favors those with typical abilities. We report on various analyses based on word predictions of a large-scale BERT language model. Statistically signiﬁcant results demonstrate that people with disabilities can be disadvantaged. Findings also explore over-lapping forms of discrimination related to in-terconnected gender and race identities.",https://www.semanticscholar.org/paper/89e5ffd5ce92011e234f0b0ea0d6f2e43647b463,10.18653/v1/2021.findings-emnlp.267,2110.00521
Stereotype and Skew: Quantifying Gender Bias in Pre-trained and Fine-tuned Language Models,background;methodology,True,2021,Daniel de Vassimon Manela;D. Errington;Thomas Fisher;B. V. Breugel;Pasquale Minervini,"This paper proposes two intuitive metrics, skew and stereotype, that quantify and analyse the gender bias present in contextual language models when tackling the WinoBias pronoun resolution task. We find evidence that gender stereotype correlates approximately negatively with gender skew in out-of-the-box models, suggesting that there is a trade-off between these two forms of bias. We investigate two methods to mitigate bias. The first approach is an online method which is effective at removing skew at the expense of stereotype. The second, inspired by previous work on ELMo, involves the fine-tuning of BERT using an augmented gender-balanced dataset. We show that this reduces both skew and stereotype relative to its unaugmented fine-tuned counterpart. However, we find that existing gender bias benchmarks do not fully probe professional bias as pronoun resolution may be obfuscated by cross-correlations from other manifestations of gender prejudice.",https://www.semanticscholar.org/paper/8fa0de4920c8edcb1fea698ff3463a347771d889,10.18653/v1/2021.eacl-main.190,2101.09688
Evaluating Metrics for Bias in Word Embeddings,background;methodology,True,2021,Sarah Schröder;Alexander Schulz;Philip Kenneweg;Robert Feldhans;Fabian Hinder;B. Hammer,"Over the last years, word and sentence embeddings have established as text preprocessing for all kinds of NLP tasks and improved the performances significantly. Unfortunately, it has also been shown that these embeddings inherit various kinds of biases from the training data and thereby pass on biases present in society to NLP solutions. Many papers attempted to quantify bias in word or sentence embeddings to evaluate debiasing methods or compare different embedding models, usually with cosine-based metrics. However, lately some works have raised doubts about these metrics showing that even though such metrics report low biases, other tests still show biases. In fact, there is a great variety of bias metrics or tests proposed in the literature without any consensus on the optimal solutions. Yet we lack works that evaluate bias metrics on a theoretical level or elaborate the advantages and disadvantages of different bias metrics. In this work, we will explore different cosine based bias metrics. We formalize a bias definition based on the ideas from previous works and derive conditions for bias metrics. Furthermore, we thoroughly investigate the existing cosine-based metrics and their limitations to show why these metrics can fail to report biases in some cases. Finally, we propose a new metric, SAME, to address the shortcomings of existing metrics and mathematically prove that SAME behaves appropriately.",https://www.semanticscholar.org/paper/93591f8d62fb962527545f0694b660b56844ef5f,,2111.07864
SynthBio: A Case Study in Human-AI Collaborative Curation of Text Datasets,,False,2021,Ann Yuan;Daphne Ippolito;Vitaly Nikolaev;Chris Callison-Burch;Andy Coenen;Sebastian Gehrmann,"NLP researchers need more, higher-quality text datasets. Human-labeled datasets are expensive to collect, while datasets collected via automatic retrieval from the web such as WikiBio [32] are noisy and can include undesired biases. Moreover, data sourced from the web is often included in datasets used to pretrain models, leading to inadvertent cross-contamination of training and test sets. In this work we introduce a novel method for efficient dataset curation: we use a large language model to provide seed generations to human raters, thereby changing dataset authoring from a writing task to an editing task. We use our method to curate SynthBio–a new evaluation set for WikiBio– composed of structured attribute lists describing fictional individuals, mapped to natural language biographies. We show that our dataset of fictional biographies is less noisy than WikiBio, and also more balanced with respect to gender and nationality.",https://www.semanticscholar.org/paper/9cc2d047b01c918168c83a48440e418471c3d666,,2111.06467
Entity-level sentiment prediction in Danmaku video interaction,,False,2021,Qingchun Bai;Kai Wei;Jie Zhou;Chao Xiong;Yuanbin Wu;Xin Lin;Liang He,,https://www.semanticscholar.org/paper/9e62ae4a8797a98c4208da795bf25d83b7de756c,10.1007/S11227-021-03652-4,
Analyzing Bias in a Knowledge-Aware News Recommendation System,,False,2021,Hannah Greven,"News recommendation systems (NRS) are increasingly shaping online news consumption and thus the world perception of many readers. In NRS, state-of-the-art deep learning models are becoming increasingly popular. In these models, unwanted biases in the training data can lead to bias in the model. In recent years, there has been a surge of research on bias in recommendation systems examining bias in word, text, and knowledge graph embeddings. Literature on analyzing biases in research models proposed for news recommendation is still scarce. To translate scientific research models into responsible NRS bias analysis is indispensable. This thesis investigates bias in the KRED NRS, a content-based knowledge-aware NRS that uses a knowledge graph as side information. To analyze KRED’s recommendations regarding political bias in exposure, we create a corpus of news articles on the politically divisive topic of migration in the EU. Using this article corpus, six news reception profiles, and synthetically generated user behavior, we analyze the diversity of recommended articles. The analysis shows that the KRED NRS preferably recommends news from news outlets more frequently contained in the user‘s reading history. Independent from the user‘s news reception profile, the KRED NRS unfairly favors news articles from Breitbart (right-wing political bias) in the article corpus.",https://www.semanticscholar.org/paper/a3501c42114edb8070a70bcd9436e1e6f1159346,,
Sexism in the Judiciary,,False,2021,Noa Baker Gillis,"We analyze 6.7 million case law documents to determine the presence of gender bias within our judicial system. We find that current bias detection methods in NLP are insufficient to determine gender bias in our case law database and propose an alternative approach. We show that existing algorithms’ inconsistent results are consequences of prior research’s inconsistent definitions of biases themselves. Bias detection algorithms rely on groups of words to represent bias (e.g., ‘salary,’ ‘job,’ and ‘boss’ to represent employment as a potentially biased theme against women in text). However, the methods to build these groups of words have several weaknesses, primarily that the word lists are based on the researchers’ own intuitions. We suggest two new methods of automating the creation of word lists to represent biases. We find that our methods outperform current NLP bias detection methods. Our research improves the capabilities of NLP technology to detect bias and highlights gender biases present in influential case law. In order to test our NLP bias detection method’s performance, we regress our results of bias in case law against U.S census data of women’s participation in the workforce in the last 100 years.",https://www.semanticscholar.org/paper/b82627c7c72dc851a436a3576ed70f3a95df1cf1,,2106.15103
What BERTs and GPTs know about your brand? Probing contextual language models for affect associations,background;methodology,True,2021,Vivek Srivastava;Stephen Pilli;S. Bhat;N. Pedanekar;Shirish S. Karande,"Investigating brand perception is fundamental to marketing strategies. In this regard, brand image, defined by a set of attributes (Aaker, 1997), is recognized as a key element in indicating how a brand is perceived by various stakeholders such as consumers and competitors. Traditional approaches (e.g., surveys) to monitor brand perceptions are time-consuming and inefficient. In the era of digital marketing, both brand managers and consumers engage with a vast amount of digital marketing content. The exponential growth of digital content has propelled the emergence of pre-trained language models such as BERT and GPT as essential tools in solving myriads of challenges with textual data. This paper seeks to investigate the extent of brand perceptions (i.e., brand and image attribute associations) these language models encode. We believe that any kind of bias for a brand and attribute pair may influence customer-centric downstream tasks such as recommender systems, sentiment analysis, and question-answering, e.g., suggesting a specific brand consistently when queried for innovative products. We use synthetic data and real-life data and report comparison results for five contextual LMs, viz. BERT, RoBERTa, DistilBERT, ALBERT and BART.",https://www.semanticscholar.org/paper/bc5f9a01a0b9e3015c094ff92f5ff1863612a2b7,10.18653/V1/2021.DEELIO-1.12,
On a Benefit of Mask Language Modeling: Robustness to Simplicity Bias,,False,2021,Ting-Rui Chiang,"Despite the success of pretrained masked language models (MLM), why MLM pretraining is useful is still a qeustion not fully answered. In this work we theoretically and empirically show that MLM pretraining makes models robust to lexicon-level spurious features, partly answer the question. We theoretically show that, when we can model the distribution of a spurious feature Π conditioned on the context, then (1) Π is at least as informative as the spurious feature, and (2) learning from Π is at least as simple as learning from the spurious feature. Therefore, MLM pretraining rescues the model from the simplicity bias caused by the spurious feature. We also explore the efficacy of MLM pretraing in causal settings. Finally we close the gap between our theories and the real world practices by conducting experiments on the hate speech detection and the name entity recognition tasks.",https://www.semanticscholar.org/paper/c0e9e14762503dd4515625c8e5d1b35436b29e9a,,2110.05301
Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you?,methodology,False,2021,Rochelle Choenni;Ekaterina Shutova;R. Rooij,"In this paper, we investigate what types of stereotypical information are captured by pretrained language models. We present the first dataset comprising stereotypical attributes of a range of social groups and propose a method to elicit stereotypes encoded by pretrained language models in an unsupervised fashion. Moreover, we link the emergent stereotypes to their manifestation as basic emotions as a means to study their emotional effects in a more generalized manner. To demonstrate how our methods can be used to analyze emotion and stereotype shifts due to linguistic experience, we use fine-tuning on news sources as a case study. Our experiments expose how attitudes towards different social groups vary across models and how quickly emotions and stereotypes can shift at the fine-tuning stage.",https://www.semanticscholar.org/paper/db742f7c4f7edd24e77482560df7d09c9033b3da,10.18653/v1/2021.emnlp-main.111,2109.10052
Pre-Trained Transformer-Based Language Models for Sundanese,methodology,False,2021,Wilson Wongso;Henry Lucky;Derwin Suhartono,"
 The Sundanese language has over 32 million speakers worldwide, but the language has reaped little to no benefits from the recent advances in natural language understanding. Like other low-resource languages, the only alternative is to fine-tune existing multilingual models. In this paper, we pre-trained three monolingual Transformer-based language models on Sundanese data. When evaluated on a downstream text classification task, we found that most of our monolingual models outperformed larger multilingual models despite the smaller overall pre-training data. In the subsequent analyses, our models benefited strongly from the Sundanese pre-training corpus size and do not exhibit socially biased behavior. We released our models for other researchers and practitioners to use.",https://www.semanticscholar.org/paper/dec42306af017bc778bbf1496776f3cd4d5bd42e,10.21203/rs.3.rs-907893/v1,
A Neural Entity Coreference Resolution Review,,False,2019,Nikolaos Stylianou;I. Vlahavas,,https://www.semanticscholar.org/paper/e0b7e97f0bddbb21cb5f6b22c3ffa08a6cf9885f,10.1016/J.ESWA.2020.114466,1910.09329
"He is very intelligent, she is very beautiful? On Mitigating Social Biases in Language Modelling and Generation",background;methodology,False,2021,Aparna Garimella;Akhash Amarnath;K. Kumar;Akash Pramod Yalla;Anandhavelu Natarajan;Niyati Chhaya;Balaji Vasan Srinivasan,"Social biases with respect to demographics (e.g., gender, age, race) in datasets are often encoded in the large pre-trained language models trained on them. Prior works have largely focused on mitigating biases in context-free representations, with recent shift to contextual ones. While this is useful for several word and sentence-level classification tasks, mitigating biases in only the representations may not suffice to use these models for language generation tasks, such as auto-completion, summarization, or dialogue generation. In this paper, we propose an approach to mitigate social biases in BERT, a large pre-trained contextual language model, and show its effectiveness in fill-in-the-blank sentence completion and summarization tasks. In addition to mitigating biases in BERT, which in general acts as an encoder, we propose lexical co-occurrence-based bias penalization in the decoder units in generation frameworks, and show bias mitigation in summarization. Finally, our approach results in better debiasing of BERT-based representations compared to post training bias mitigation, thus illustrating the efficacy of our approach to not just mitigate biases in representations, but also generate text with reduced biases.",https://www.semanticscholar.org/paper/ea667d3f5df2954c7365b8d1218889e2fc514829,10.18653/v1/2021.findings-acl.397,
Simple Entity-Centric Questions Challenge Dense Retrievers,methodology,False,2021,Christopher Sciavolino;Zexuan Zhong;Jinhyuk Lee;Danqi Chen,"Open-domain question answering has exploded in popularity recently due to the success of dense retrieval models, which have surpassed sparse models using only a few supervised training examples. However, in this paper, we demonstrate current dense models are not yet the holy grail of retrieval. We first construct EntityQuestions, a set of simple, entity-rich questions based on facts from Wikidata (e.g., “Where was Arve Furset born?”), and observe that dense retrievers drastically under-perform sparse methods. We investigate this issue and uncover that dense retrievers can only generalize to common entities unless the question pattern is explicitly observed during training. We discuss two simple solutions towards addressing this critical problem. First, we demonstrate that data augmentation is unable to fix the generalization problem. Second, we argue a more robust passage encoder helps facilitate better question adaptation using specialized question encoders. We hope our work can shed light on the challenges in creating a robust, universal dense retriever that works well across different input distributions.",https://www.semanticscholar.org/paper/f7a6b57adebb5f6a10d16e120f0b0ef55aab7b2b,10.18653/v1/2021.emnlp-main.496,2109.08535
Detecting Independent Pronoun Bias with Partially-Synthetic Data Generation,methodology,False,2020,R. Munro;Alex (Carmen) Morrison,"We report that state-of-the-art parsers consistently failed to identify “hers” and “theirs” as pronouns but identified the masculine equivalent “his”. We find that the same biases exist in recent language models like BERT. While some of the bias comes from known sources, like training data with gender imbalances, we find that the bias is _amplified_ in the language models and that linguistic differences between English pronouns that are not inherently biased can become biases in some machine learning models. We introduce a new technique for measuring bias in models, using Bayesian approximations to generate partially-synthetic data from the model itself.",https://www.semanticscholar.org/paper/055c04900d65bdcacdd0cc98a782e34f9bd7a544,10.18653/v1/2020.emnlp-main.157,
Unmasking Contextual Stereotypes: Measuring and Mitigating BERT's Gender Bias,background;methodology,True,2020,Marion Bartl;M. Nissim;Albert Gatt,"Contextualized word embeddings have been replacing standard embeddings as the representational knowledge source of choice in NLP systems. Since a variety of biases have previously been found in standard word embeddings, it is crucial to assess biases encoded in their replacements as well. Focusing on BERT (Devlin et al., 2018), we measure gender bias by studying associations between gender-denoting target words and names of professions in English and German, comparing the findings with real-world workforce statistics. We mitigate bias by fine-tuning BERT on the GAP corpus (Webster et al., 2018), after applying Counterfactual Data Substitution (CDS) (Maudslay et al., 2019). We show that our method of measuring bias is appropriate for languages such as English, but not for languages with a rich morphology and gender-marking, such as German. Our results highlight the importance of investigating bias and mitigation techniques cross-linguistically, especially in view of the current emphasis on large-scale, multilingual language models.",https://www.semanticscholar.org/paper/0712334d1109248e52706f13aeff5281834727f8,,2010.14534
Towards Debiasing Sentence Representations,,False,2020,Paul Pu Liang;Irene Z Li;Emily Zheng;Y. Lim;R. Salakhutdinov;Louis-Philippe Morency,"As natural language processing methods are increasingly deployed in real-world scenarios such as healthcare, legal systems, and social science, it becomes necessary to recognize the role they potentially play in shaping social biases and stereotypes. Previous work has revealed the presence of social biases in widely used word embeddings involving gender, race, religion, and other social constructs. While some methods were proposed to debias these word-level embeddings, there is a need to perform debiasing at the sentence-level given the recent shift towards new contextualized sentence representations such as ELMo and BERT. In this paper, we investigate the presence of social biases in sentence-level representations and propose a new method, Sent-Debias, to reduce these biases. We show that Sent-Debias is effective in removing biases, and at the same time, preserves performance on sentence-level downstream tasks such as sentiment analysis, linguistic acceptability, and natural language understanding. We hope that our work will inspire future research on characterizing and removing social biases from widely adopted sentence representations for fairer NLP.",https://www.semanticscholar.org/paper/0d965ed237a3b4592ecefdb618c29f63adedff76,10.18653/v1/2020.acl-main.488,2007.08100
Evaluating representations by the complexity of learning low-loss predictors,,False,2020,William F. Whitney;M. Song;David Brandfonbrener;Jaan Altosaar;Kyunghyun Cho,"We consider the problem of evaluating representations of data for use in solving a downstream task. We propose to measure the quality of a representation by the complexity of learning a predictor on top of the representation that achieves low loss on a task of interest. To this end, we introduce two measures: surplus description length (SDL) and e sample complexity (eSC). To compare our methods to prior work, we also present a framework based on plotting the validation loss versus dataset size (the ""loss-data"" curve). Existing measures, such as mutual information and minimum description length, correspond to slices and integrals along the data-axis of the loss-data curve, while ours correspond to slices and integrals along the loss-axis. This analysis shows that prior methods measure properties of an evaluation dataset of a specified size, whereas our methods measure properties of a predictor with a specified loss. We conclude with experiments on real data to compare the behavior of these methods over datasets of varying size.",https://www.semanticscholar.org/paper/15aa86556be1579eadf8fbb0bc486f9427e681f0,,2009.07368
Unequal Representations: Analyzing Intersectional Biases in Word Embeddings Using Representational Similarity Analysis,result,False,2020,Michael A. Lepori,"We present a new approach for detecting human-like social biases in word embeddings using representational similarity analysis. Specifically, we probe contextualized and non-contextualized embeddings for evidence of intersectional biases against Black women. We show that these embeddings represent Black women as simultaneously less feminine than White women, and less Black than Black men. This finding aligns with intersectionality theory, which argues that multiple identity categories (such as race or sex) layer on top of each other in order to create unique modes of discrimination that are not shared by any individual category.",https://www.semanticscholar.org/paper/1a0e746a185ffbcbbd9e2e7f8404dc03d89852d9,10.18653/V1/2020.COLING-MAIN.151,2011.12086
Model Choices Influence Attributive Word Associations: A Semi-supervised Analysis of Static Word Embeddings,methodology,False,2020,Geetanjali Bihani;Julia Taylor Rayz,"Static word embeddings encode word associations, extensively utilized in downstream NLP tasks. Although prior studies have discussed the nature of such word associations in terms of biases and lexical regularities captured, the variation in word associations based on the embedding training procedure remains in obscurity. This work aims to address this gap by assessing attributive word associations across five different static word embedding architectures, analyzing the impact of the choice of the model architecture, context learning flavor and training corpora. Our approach utilizes a semi-supervised clustering method to cluster annotated proper nouns and adjectives, based on their word embedding features, revealing underlying attributive word associations formed in the embedding space, without introducing any confirmation bias. Our results reveal that the choice of the context learning flavor during embedding training (CBOW vs skip-gram) impacts the word association distinguishability and word embeddings’ sensitivity to deviations in the training corpora. Moreover, it is empirically shown that even when trained over the same corpora, there is significant inter-model disparity and intra-model similarity in the encoded word associations across different word embedding models, portraying specific patterns in the way the embedding space is created for each embedding architecture.",https://www.semanticscholar.org/paper/2aced7f246f09fc47d8bdcd1a895ef900ee2f318,10.1109/WIIAT50758.2020.00085,2012.07978
Towards Ethics by Design in Online Abusive Content Detection,methodology,False,2020,Svetlana Kiritchenko;I. Nejadgholi,"To support safety and inclusion in online communications, significant efforts in NLP research have been put towards addressing the problem of abusive content detection, commonly defined as a supervised classification task. The research effort has spread out across several closely related sub-areas, such as detection of hate speech, toxicity, cyberbullying, etc. There is a pressing need to consolidate the field under a common framework for task formulation, dataset design and performance evaluation. Further, despite current technologies achieving high classification accuracies, several ethical issues have been revealed. We bring ethical issues to forefront and propose a unified framework as a two-step process. First, online content is categorized around personal and identity-related subject matters. Second, severity of abuse is identified through comparative annotation within each category. The novel framework is guided by the Ethics by Design principle and is a step towards building more accurate and trusted models.",https://www.semanticscholar.org/paper/30b923111a5c8ec27bf9a409d10daf614d46435b,,2010.14952
Social Biases in NLP Models as Barriers for Persons with Disabilities,methodology,False,2020,B. Hutchinson;Vinodkumar Prabhakaran;Emily L. Denton;Kellie Webster;Yu Zhong;Stephen Denuyl,"Building equitable and inclusive NLP technologies demands consideration of whether and how social attitudes are represented in ML models. In particular, representations encoded in models often inadvertently perpetuate undesirable social biases from the data on which they are trained. In this paper, we present evidence of such undesirable biases towards mentions of disability in two different English language models: toxicity prediction and sentiment analysis. Next, we demonstrate that the neural embeddings that are the critical first step in most NLP pipelines similarly contain undesirable biases towards mentions of disability. We end by highlighting topical biases in the discourse about disability which may contribute to the observed model biases; for instance, gun violence, homelessness, and drug addiction are over-represented in texts discussing mental illness.",https://www.semanticscholar.org/paper/3cc2f69951cd24fe61be4cf32d62afbac297bc2b,10.18653/v1/2020.acl-main.487,2005.00813
"Situated Data, Situated Systems: A Methodology to Engage with Power Relations in Natural Language Processing Research",methodology,False,2020,Lucy Havens;Melissa Mhairi Terras;B. Bach;Beatrice Alex,"We propose a bias-aware methodology to engage with power relations in natural language processing (NLP) research. NLP research rarely engages with bias in social contexts, limiting its ability to mitigate bias. While researchers have recommended actions, technical methods, and documentation practices, no methodology exists to integrate critical reflections on bias with technical NLP methods. In this paper, after an extensive and interdisciplinary literature review, we contribute a bias-aware methodology for NLP research. We also contribute a definition of biased text, a discussion of the implications of biased NLP systems, and a case study demonstrating how we are executing the bias-aware methodology in research on archival metadata descriptions.",https://www.semanticscholar.org/paper/59faa64d2681b6257ca5a87bc28d7d535b8c8809,,2011.05911
RobBERT: a Dutch RoBERTa-based Language Model,background;methodology,False,2020,Pieter Delobelle;Thomas Winters;Bettina Berendt,"Pre-trained language models have been dominating the field of natural language processing in recent years, and have led to significant performance gains for various complex natural language tasks. One of the most prominent pre-trained language models is BERT, which was released as an English as well as a multilingual version. Although multilingual BERT performs well on many tasks, recent studies show that BERT models trained on a single language significantly outperform the multilingual version. Training a Dutch BERT model thus has a lot of potential for a wide range of Dutch NLP tasks. While previous approaches have used earlier implementations of BERT to train a Dutch version of BERT, we used RoBERTa, a robustly optimized BERT approach, to train a Dutch language model called RobBERT. We measured its performance on various tasks as well as the importance of the fine-tuning dataset size. We also evaluated the importance of language-specific tokenizers and the model’s fairness. We found that RobBERT improves state-of-the-art results for various tasks, and especially significantly outperforms other models when dealing with smaller datasets. These results indicate that it is a powerful pre-trained model for a large variety of Dutch language tasks. The pre-trained and fine-tuned models are publicly available to support further downstream Dutch NLP applications.",https://www.semanticscholar.org/paper/634e8ee7e86f253c4b6c722a3bb7c32b7aa3892b,10.18653/v1/2020.findings-emnlp.292,2001.06286
Multi-Dimensional Gender Bias Classification,,False,2020,Emily Dinan;Angela Fan;Ledell Yu Wu;J. Weston;Douwe Kiela;Adina Williams,"Machine learning models are trained to find patterns in data. NLP models can inadvertently learn socially undesirable patterns when training on gender biased text. In this work, we propose a general framework that decomposes gender bias in text along several pragmatic and semantic dimensions: bias from the gender of the person being spoken about, bias from the gender of the person being spoken to, and bias from the gender of the speaker. Using this fine-grained framework, we automatically annotate eight large scale datasets with gender information. In addition, we collect a novel, crowdsourced evaluation benchmark of utterance-level gender rewrites. Distinguishing between gender bias along multiple dimensions is important, as it enables us to train finer-grained gender bias classifiers. We show our classifiers prove valuable for a variety of important applications, such as controlling for gender bias in generative models, detecting gender bias in arbitrary text, and shed light on offensive language in terms of genderedness.",https://www.semanticscholar.org/paper/ad9d93406f3cf3ffe5a640cb4d742f202339a511,10.18653/v1/2020.emnlp-main.23,2005.00614
Double-Hard Debias: Tailoring Word Embeddings for Gender Bias Mitigation,,False,2020,Tianlu Wang;Xi Victoria Lin;Nazneen Rajani;Vicente Ordonez;Caimng Xiong,"Word embeddings derived from human-generated corpora inherit strong gender bias which can be further amplified by downstream models. Some commonly adopted debiasing approaches, including the seminal Hard Debias algorithm, apply post-processing procedures that project pre-trained word embeddings into a subspace orthogonal to an inferred gender subspace. We discover that semantic-agnostic corpus regularities such as word frequency captured by the word embeddings negatively impact the performance of these algorithms. We propose a simple but effective technique, Double Hard Debias, which purifies the word embeddings against such corpus regularities prior to inferring and removing the gender subspace. Experiments on three bias mitigation benchmarks show that our approach preserves the distributional semantics of the pre-trained word embeddings while reducing gender bias to a significantly larger degree than prior approaches.",https://www.semanticscholar.org/paper/b58abb51a5dae7fb753be4535e468a6f1f07f873,10.18653/v1/2020.acl-main.484,2005.00965
Monolingual and Multilingual Reduction of Gender Bias in Contextualized Representations,methodology,False,2020,Sheng Liang;Philipp Dufter;Hinrich Schütze,"Pretrained language models (PLMs) learn stereotypes held by humans and reflected in text from their training corpora, including gender bias. When PLMs are used for downstream tasks such as picking candidates for a job, people’s lives can be negatively affected by these learned stereotypes. Prior work usually identifies a linear gender subspace and removes gender information by eliminating the subspace. Following this line of work, we propose to use DensRay, an analytical method for obtaining interpretable dense subspaces. We show that DensRay performs on-par with prior approaches, but provide arguments that it is more robust and provide indications that it preserves language model performance better. By applying DensRay to attention heads and layers of BERT we show that gender information is spread across all attention heads and most of the layers. Also we show that DensRay can obtain gender bias scores on both token and sentence levels. Finally, we demonstrate that we can remove bias multilingually, e.g., from Chinese, using only English training data.",https://www.semanticscholar.org/paper/c1035a3fa5a0c74dea099b515ecc58d2d9387bac,10.5282/UBM/EPUB.74040,
Towards Interpreting BERT for Reading Comprehension Based QA,,False,2020,Sahana Ramnath;Preksha Nema;Deep Sahni;Mitesh M. Khapra,"BERT and its variants have achieved state-of-the-art performance in various NLP tasks. Since then, various works have been proposed to analyze the linguistic information being captured in BERT. However, the current works do not provide an insight into how BERT is able to achieve near human-level performance on the task of Reading Comprehension based Question Answering. In this work, we attempt to interpret BERT for RCQA. Since BERT layers do not have predefined roles, we define a layer's role or functionality using Integrated Gradients. Based on the defined roles, we perform a preliminary analysis across all layers. We observed that the initial layers focus on query-passage interaction, whereas later layers focus more on contextual understanding and enhancing the answer prediction. Specifically for quantifier questions (how much/how many), we notice that BERT focuses on confusing words (i.e., on other numerical quantities in the passage) in the later layers, but still manages to predict the answer correctly. The fine-tuning and analysis scripts will be publicly available at this https URL .",https://www.semanticscholar.org/paper/c4788233436ccfc664a8293c78447be2827c9088,10.18653/v1/2020.emnlp-main.261,2010.08983
Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings,methodology,False,2020,Rishi Bommasani;Kelly Davis;Claire Cardie,"Contextualized representations (e.g. ELMo, BERT) have become the default pretrained representations for downstream NLP applications. In some settings, this transition has rendered their static embedding predecessors (e.g. Word2Vec, GloVe) obsolete. As a side-effect, we observe that older interpretability methods for static embeddings — while more diverse and mature than those available for their dynamic counterparts — are underutilized in studying newer contextualized representations. Consequently, we introduce simple and fully general methods for converting from contextualized representations to static lookup-table embeddings which we apply to 5 popular pretrained models and 9 sets of pretrained weights. Our analysis of the resulting static embeddings notably reveals that pooling over many contexts significantly improves representational quality under intrinsic evaluation. Complementary to analyzing representational quality, we consider social biases encoded in pretrained representations with respect to gender, race/ethnicity, and religion and find that bias is encoded disparately across pretrained models and internal layers even for models with the same training data. Concerningly, we find dramatic inconsistencies between social bias estimators for word embeddings.",https://www.semanticscholar.org/paper/d34580c522c79d5cde620331dd9ffb18643a8090,10.18653/v1/2020.acl-main.431,
Hurtful words: quantifying biases in clinical contextual word embeddings,background;methodology,True,2020,H. Zhang;Amy X. Lu;Mohamed Abdalla;Matthew B. A. McDermott;M. Ghassemi,"In this work, we examine the extent to which embeddings may encode marginalized populations differently, and how this may lead to a perpetuation of biases and worsened performance on clinical tasks. We pretrain deep embedding models (BERT) on medical notes from the MIMIC-III hospital dataset, and quantify potential disparities using two approaches. First, we identify dangerous latent relationships that are captured by the contextual word embeddings using a fill-in-the-blank method with text from real clinical notes and a log probability bias score quantification. Second, we evaluate performance gaps across different definitions of fairness on over 50 downstream clinical prediction tasks that include detection of acute and chronic conditions. We find that classifiers trained from BERT representations exhibit statistically significant differences in performance, often favoring the majority group with regards to gender, language, ethnicity, and insurance status. Finally, we explore shortcomings of using adversarial debiasing to obfuscate subgroup information in contextual word embeddings, and recommend best practices for such deep embedding models in clinical settings.",https://www.semanticscholar.org/paper/d505eb794676927e919bcfeafdd1680a4bf10229,10.1145/3368555.3384448,2003.11515
Predictive Biases in Natural Language Processing Models: A Conceptual Framework and Overview,background;methodology,True,2019,Deven Santosh Shah;H. A. Schwartz;Dirk Hovy,"An increasing number of natural language processing papers address the effect of bias on predictions, introducing mitigation techniques at different parts of the standard NLP pipeline (data and models). However, these works have been conducted individually, without a unifying framework to organize efforts within the field. This situation leads to repetitive approaches, and focuses overly on bias symptoms/effects, rather than on their origins, which could limit the development of effective countermeasures. In this paper, we propose a unifying predictive bias framework for NLP. We summarize the NLP literature and suggest general mathematical definitions of predictive bias. We differentiate two consequences of bias: outcome disparities and error disparities, as well as four potential origins of biases: label bias, selection bias, model overamplification, and semantic bias. Our framework serves as an overview of predictive bias in NLP, integrating existing work into a single structure, and providing a conceptual baseline for improved frameworks.",https://www.semanticscholar.org/paper/eef4df3a5232c7ce70123aaebb326ff9169a3c8c,10.18653/v1/2020.acl-main.468,1912.11078
On the Unintended Social Bias of Training Language Generation Models with Data from Local Media,methodology,False,2019,Omar U. Florez,"There are concerns that neural language models may preserve some of the stereotypes of the underlying societies that generate the large corpora needed to train these models. For example, gender bias is a significant problem when generating text, and its unintended memorization could impact the user experience of many applications (e.g., the smart-compose feature in Gmail). 
In this paper, we introduce a novel architecture that decouples the representation learning of a neural model from its memory management role. This architecture allows us to update a memory module with an equal ratio across gender types addressing biased correlations directly in the latent space. We experimentally show that our approach can mitigate the gender bias amplification in the automatic generation of articles news while providing similar perplexity values when extending the Sequence2Sequence architecture.",https://www.semanticscholar.org/paper/7ec8fc4d2caaf8104214f66b40e84c9a542be481,,1911.00461
Entity-Centric Contextual Affective Analysis,result,False,2019,Anjalie Field;Yulia Tsvetkov,"While contextualized word representations have improved state-of-the-art benchmarks in many NLP tasks, their potential usefulness for social-oriented tasks remains largely unexplored. We show how contextualized word embeddings can be used to capture affect dimensions in portrayals of people. We evaluate our methodology quantitatively, on held-out affect lexicons, and qualitatively, through case examples. We find that contextualized word representations do encode meaningful affect information, but they are heavily biased towards their training data, which limits their usefulness to in-domain analyses. We ultimately use our method to examine differences in portrayals of men and women.",https://www.semanticscholar.org/paper/a1280728623e8fd605284b2b7cf536579b9e2cbf,10.18653/v1/P19-1243,1906.01762
The Role of Protected Class Word Lists in Bias Identification of Contextualized Word Representations,background;methodology,False,2019,João Sedoc;L. Ungar,"Systemic bias in word embeddings has been widely reported and studied, and efforts made to debias them; however, new contextualized embeddings such as ELMo and BERT are only now being similarly studied. Standard debiasing methods require heterogeneous lists of target words to identify the “bias subspace”. We show show that using new contextualized word embeddings in conceptor debiasing allows us to more accurately debias word embeddings by breaking target word lists into more homogeneous subsets and then combining (”Or’ing”) the debiasing conceptors of the different subsets.",https://www.semanticscholar.org/paper/dd20fc43fa2a3d32eb85d4520d3c34c7ce4e41b9,10.18653/v1/W19-3808,
Impact of Spanish Dialect in Deep Learning Next Sentence Predictors,,False,2019,P. Duboué,"With the second largest number of native speakers in the world, Spanish exhibits a large dialectical difference, both in grammar and word forms. We analyze the impact of these differences in a Natural Language Processing task using the proceedings of two parliamentary bodies from Costa Rica and Argentina. The task chosen is to determine whether to pieces of text are coherent, that is, whether they can follow each other in normal discourse. We train a deep learning network to perform next sentence prediction using the BERT model. Our experiments indicate that task adaptation presents a more substantive impact than regional differences but dialect still can account for up to 8% difference in F-measure.",https://www.semanticscholar.org/paper/f1df9e42869ae47cb5080cafb31905edce50ac31,,
